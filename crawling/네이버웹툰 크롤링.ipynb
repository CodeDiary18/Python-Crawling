{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버웹툰 크롤링\n",
    "### chromedrive.exe의 위치는 이 주피터 노트북이 있는 디렉토리 안에 있어야 함\n",
    "### [참고](https://yeeybook.tistory.com/139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://comic.naver.com/webtoon/weekday.nhn'\n",
    "html = requests.get(URL).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find_all('a', {'class' : 'title'}) # a태그에서 class='title'인 html소스를 찾아 할당\n",
    "id_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "day_list = []\n",
    "genre_list = []\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title)):\n",
    "    sleep(1)\n",
    "    page = driver.find_elements_by_class_name('title')\n",
    "    page[i].click() #월요일 첫 번째 웹툰부터 순서대로 클릭\n",
    "    sleep(1)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser') # 이동한 페이지 주소 읽고 파싱\n",
    "\n",
    "    day = soup.find_all('ul', {'class' : 'category_tab'})\n",
    "    day = day[0].find('li', {'class' : 'on'}).text[0:1] # 요일 수집\n",
    "\n",
    "    t = title[i].text\n",
    "    if (t in title_list):  # 요일 두 개 이상이면 요일만 추가함\n",
    "        day_list[title_list.index(t)] += ', ' + day\n",
    "        driver.back()\n",
    "        continue\n",
    "    id_list.append(i) ; num += 1  # id 리스트에 추가\n",
    "    title_list.append(t)  # 제목 리스트에 추가\n",
    "    day_list.append(day) # 요일 리스트에 추가\n",
    "\n",
    "    author = soup.find_all('h2') # 두 번째 h2태그에 있음\n",
    "    author = author[1].find('span', {'class' : 'wrt_nm'}).text[8:] # 7칸의 공백 후 8번 째부터 작가 이름임\n",
    "    author_list.append(author) # 작가 리스트에 추가\n",
    "\n",
    "    genre = soup.find('span', {'class' : 'genre'}).text # 장르 수집\n",
    "    genre_list.append(genre) # 장르 리스트에 추가\n",
    "    driver.back() # 뒤로 가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "df = pd.DataFrame(columns = cols)\n",
    "df['id'] = id_list\n",
    "df['title'] = title_list\n",
    "df['author'] = author_list\n",
    "df['day'] = day_list\n",
    "df['genre'] = genre_list\n",
    "df.to_csv('네이버_웹툰.csv', encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38tensorflow]",
   "language": "python",
   "name": "conda-env-py38tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
